{"block_file": {"data_exporters/load_postgres.py:data_exporter:python:load postgres": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\nfrom sqlalchemy import create_engine\nfrom dotenv import load_dotenv\nimport os\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(data: DataFrame, **kwargs) -> None:\n  \n        # Get database connection parameters from environment variables\n        POSTGRES_USER = os.getenv('POSTGRES_USER', \"postgres\")\n        POSTGRES_PASSWORD =   os.getenv('POSTGRES_PASSWORD', \"postgres\")\n        POSTGRES_DB = os.getenv('POSTGRES_DB',\"mlops_project\")\n        POSTGRES_HOST =  os.getenv('POSTGRES_HOST', 'postgres')\n        POSTGRES_PORT =  os.getenv('POSTGRES_PORT', '5432')\n\n        # Create the database connection URL\n        DATABASE_URL = f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}\"\n\n        # Create a SQLAlchemy engine\n        engine = create_engine(DATABASE_URL)\n\n        try:\n            data.to_sql('energy_data', engine, if_exists='replace', index=False)\n            # if_exists='replace' => avodid bugg during dev.... to change \n            return \"DataFrame exported successfully to PostgreSQL.\"\n        except Exception as e:\n            return f\"An error occurred: {e}\"\n\n\n\n", "file_path": "data_exporters/load_postgres.py", "language": "python", "type": "data_exporter", "uuid": "load_postgres"}, "data_loaders/extract_input_data.py:data_loader:python:extract input data": {"content": "import pandas as pd\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n\n@data_loader\ndef data_preparation():\n    years=[2022,2023]\n    all_years=None\n    for year in years:\n        this_year = get_input_data(year)\n        assert this_year.shape[0]== 365, f\"Data issue => not 365 days in yer {year}\"\n        all_years=  pd.concat([all_years, this_year])\n\n    all_years.set_index('date', drop= False, inplace = True)\n    all_years.sort_index(inplace = True)\n    all_years = all_years.asfreq('D', method = 'bfill')\n    return all_years\n\ndef get_input_data( year: int):\n\n    daily_cons = pd.read_excel(f'https://github.com/remitoudic/mlops/raw/main/project/data%20/consumption/daily_{year}.xls', skiprows=17,  usecols=lambda x: x if not x.startswith('Unnamed') else None)\n    daily_cons.dropna(inplace=True)\n    daily_cons.reset_index(inplace=True,drop=True)\n    daily_cons.drop(['Type de donn\u00e9es'],inplace=True, axis=1)\n    daily_cons.rename(columns={'Energie journali\u00e8re (MWh)': \"MWH\"},inplace=True)\n    daily_cons.rename(columns={'Date': \"date\"},inplace=True)\n    daily_cons['date'] = pd.to_datetime(daily_cons['date'], format=\"%d/%m/%Y\", errors='coerce')\n    #daily_cons['date']=daily_cons['date'].dt.strftime('%m/%d/%Y')\n    return daily_cons\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/extract_input_data.py", "language": "python", "type": "data_loader", "uuid": "extract_input_data"}, "data_loaders/ingest_data.py:data_loader:python:ingest data": {"content": "import pandas as pd\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef data_preparation():\n    years=[2022,2023]\n    all_years=None\n    for year in years:\n        this_year = get_input_data(year)\n        assert this_year.shape[0]== 365, f\"Data issue => not 365 days in yer {year}\"\n        all_years=  pd.concat([all_years, this_year])\n\n    all_years.set_index('date', drop= False, inplace = True)\n    all_years.sort_index(inplace = True)\n    all_years = all_years.asfreq('D', method = 'bfill')\n    return all_years\n\ndef get_input_data( year: int):\n\n    daily_cons = pd.read_excel(f'https://github.com/remitoudic/mlops/raw/main/project/data%20/consumption/daily_{year}.xls', skiprows=17,  usecols=lambda x: x if not x.startswith('Unnamed') else None)\n    daily_cons.dropna(inplace=True)\n    daily_cons.reset_index(inplace=True,drop=True)\n    daily_cons.drop(['Type de donn\u00e9es'],inplace=True, axis=1)\n    daily_cons.rename(columns={'Energie journali\u00e8re (MWh)': \"MWh\"},inplace=True)\n    daily_cons.rename(columns={'Date': \"date\"},inplace=True)\n    daily_cons['date'] = pd.to_datetime(daily_cons['date'], format=\"%d/%m/%Y\", errors='coerce')\n    #daily_cons['date']=daily_cons['date'].dt.strftime('%m/%d/%Y')\n    return daily_cons\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/ingest_data.py", "language": "python", "type": "data_loader", "uuid": "ingest_data"}, "transformers/transform_input_data.py:transformer:python:transform input data": {"content": "import pandas as pd\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    # format and split data \n    #data['date'] = pd.to_datetime(data[\"date\"].dt.strftime('%Y-%m'))\n    #\n    # split_step = 30\n    # result ={ 'data_train': data[:-split_step],\n    #          'data_test' :data[-split_step:]\n    #         }\n    return data\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/transform_input_data.py", "language": "python", "type": "transformer", "uuid": "transform_input_data"}, "transformers/pre_process_input_date.py:transformer:python:pre process input date": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    # format and split data \n    data['date'] = pd.to_datetime(data[\"date\"].dt.strftime('%Y-%m'))\n    #\n    split_step = 30\n    result={ 'data_train': data[:-split_step],\n             'data_test' :data[-split_step:]\n            }\n    return data\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/pre_process_input_date.py", "language": "python", "type": "transformer", "uuid": "pre_process_input_date"}, "pipelines/mlops_project/metadata.yaml:pipeline:yaml:mlops project/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_loaders/extract_input_data.py\n    file_source:\n      path: data_loaders/extract_input_data.py\n  downstream_blocks:\n  - transform_input_data\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: EXTRACT_input_data\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: extract_input_data\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: transformers/transform_input_data.py\n    file_source:\n      path: transformers/transform_input_data.py\n  downstream_blocks:\n  - load_postgres\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: transform_input_data\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - extract_input_data\n  uuid: transform_input_data\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_exporters/load_postgres.py\n    file_source:\n      path: data_exporters/load_postgres.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: Load_postgres\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - transform_input_data\n  uuid: load_postgres\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: null\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: null\nnotification_config:\n  alert_on:\n  - trigger_failure\n  - trigger_passed_sla\n  slack_config:\n    webhook_url: '{{ env_var(''MAGE_SLACK_WEBHOOK_URL'') }}'\n  teams_config:\n    webhook_url: '{{ env_var(''MAGE_TEAMS_WEBHOOK_URL'') }}'\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config:\n  app_name: my spark app\n  custom_session_var_name: spark\n  executor_env: {}\n  others: {}\n  spark_home: null\n  spark_jars: []\n  spark_master: local\n  use_custom_session: false\ntags: []\ntype: python\nuuid: mlops_project\nvariables_dir: /home/src/mage_data/src\nwidgets: []\n", "file_path": "pipelines/mlops_project/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "mlops_project/metadata"}, "pipelines/mlops_project/__init__.py:pipeline:python:mlops project/  init  ": {"content": "", "file_path": "pipelines/mlops_project/__init__.py", "language": "python", "type": "pipeline", "uuid": "mlops_project/__init__"}, "pipelines/mlops_project/io_config.yaml:pipeline:yaml:mlops project/io config": {"content": "version: 0.1.1\ndefault:\n  # Default profile created for data IO access.\n  # Add your credentials for the source you use, and delete the rest.\n  # AWS\n  AWS_ACCESS_KEY_ID: \"{{ env_var('AWS_ACCESS_KEY_ID') }}\"\n  AWS_SECRET_ACCESS_KEY: \"{{ env_var('AWS_SECRET_ACCESS_KEY') }}\"\n  AWS_SESSION_TOKEN: session_token (Used to generate Redshift credentials)\n  AWS_REGION: region\n  # Algolia\n  ALGOLIA_APP_ID: app_id\n  ALGOLIA_API_KEY: api_key\n  ALGOLIA_INDEX_NAME: index_name\n  # Azure\n  AZURE_CLIENT_ID: \"{{ env_var('AZURE_CLIENT_ID') }}\"\n  AZURE_CLIENT_SECRET: \"{{ env_var('AZURE_CLIENT_SECRET') }}\"\n  AZURE_STORAGE_ACCOUNT_NAME: \"{{ env_var('AZURE_STORAGE_ACCOUNT_NAME') }}\"\n  AZURE_TENANT_ID: \"{{ env_var('AZURE_TENANT_ID') }}\"\n  # Chroma\n  CHROMA_COLLECTION: collection_name\n  CHROMA_PATH: path\n  # Clickhouse\n  CLICKHOUSE_DATABASE: default\n  CLICKHOUSE_HOST: host.docker.internal\n  CLICKHOUSE_INTERFACE: http\n  CLICKHOUSE_PASSWORD: null\n  CLICKHOUSE_PORT: 8123\n  CLICKHOUSE_USERNAME: null\n  # Druid\n  DRUID_HOST: hostname\n  DRUID_PASSWORD: password\n  DRUID_PATH: /druid/v2/sql/\n  DRUID_PORT: 8082\n  DRUID_SCHEME: http\n  DRUID_USER: user\n  # DuckDB\n  DUCKDB_DATABASE: database\n  DUCKDB_SCHEMA: main\n  # Google\n  GOOGLE_SERVICE_ACC_KEY:\n    type: service_account\n    project_id: project-id\n    private_key_id: key-id\n    private_key: \"-----BEGIN PRIVATE KEY-----\\nyour_private_key\\n-----END_PRIVATE_KEY\"\n    client_email: your_service_account_email\n    auth_uri: \"https://accounts.google.com/o/oauth2/auth\"\n    token_uri: \"https://accounts.google.com/o/oauth2/token\"\n    auth_provider_x509_cert_url: \"https://www.googleapis.com/oauth2/v1/certs\"\n    client_x509_cert_url: \"https://www.googleapis.com/robot/v1/metadata/x509/your_service_account_email\"\n  GOOGLE_SERVICE_ACC_KEY_FILEPATH: \"/path/to/your/service/account/key.json\"\n  GOOGLE_LOCATION: US # Optional\n  # MongoDB\n  # Specify either the connection string or the (host, password, user, port) to connect to MongoDB.\n  MONGODB_CONNECTION_STRING: \"mongodb://{username}:{password}@{host}:{port}/\"\n  MONGODB_HOST: host\n  MONGODB_PORT: 27017\n  MONGODB_USER: user\n  MONGODB_PASSWORD: password\n  MONGODB_DATABASE: database\n  MONGODB_COLLECTION: collection\n  # MSSQL\n  MSSQL_DATABASE: database\n  MSSQL_SCHEMA: schema\n  MSSQL_DRIVER: \"ODBC Driver 18 for SQL Server\"\n  MSSQL_HOST: host\n  MSSQL_PASSWORD: password\n  MSSQL_PORT: 1433\n  MSSQL_USER: SA\n  # MySQL\n  MYSQL_DATABASE: database\n  MYSQL_HOST: host\n  MYSQL_PASSWORD: password\n  MYSQL_PORT: 3306\n  MYSQL_USER: root\n  # Pinot\n  PINOT_HOST: hostname\n  PINOT_PASSWORD: password\n  PINOT_PATH: /query/sql\n  PINOT_PORT: 8000\n  PINOT_SCHEME: http\n  PINOT_USER: user\n  # PostgresSQL\n  POSTGRES_CONNECT_TIMEOUT: 10\n  POSTGRES_DBNAME: postgres\n  POSTGRES_SCHEMA: public # Optional\n  POSTGRES_USER: username\n  POSTGRES_PASSWORD: password\n  POSTGRES_HOST: hostname\n  POSTGRES_PORT: 5432\n  # Qdrant\n  QDRANT_COLLECTION: collection\n  QDRANT_PATH: path\n  # Redshift\n  REDSHIFT_SCHEMA: public # Optional\n  REDSHIFT_DBNAME: redshift_db_name\n  REDSHIFT_HOST: redshift_cluster_id.identifier.region.redshift.amazonaws.com\n  REDSHIFT_PORT: 5439\n  REDSHIFT_TEMP_CRED_USER: temp_username\n  REDSHIFT_TEMP_CRED_PASSWORD: temp_password\n  REDSHIFT_DBUSER: redshift_db_user\n  REDSHIFT_CLUSTER_ID: redshift_cluster_id\n  REDSHIFT_IAM_PROFILE: default\n  # Snowflake\n  SNOWFLAKE_USER: username\n  SNOWFLAKE_PASSWORD: password\n  SNOWFLAKE_ACCOUNT: account_id.region\n  SNOWFLAKE_DEFAULT_WH: null                  # Optional default warehouse\n  SNOWFLAKE_DEFAULT_DB: null                  # Optional default database\n  SNOWFLAKE_DEFAULT_SCHEMA: null              # Optional default schema\n  SNOWFLAKE_PRIVATE_KEY_PASSPHRASE: null      # Optional private key passphrase\n  SNOWFLAKE_PRIVATE_KEY_PATH: null            # Optional private key path\n  SNOWFLAKE_ROLE: null                        # Optional role name\n  SNOWFLAKE_TIMEOUT: null                     # Optional timeout in seconds\n  # Trino\n  trino:\n    catalog: postgresql                       # Change this to the catalog of your choice\n    host: 127.0.0.1\n    http_headers:\n      X-Something: 'mage=power'\n    http_scheme: http\n    password: mage1337                        # Optional\n    port: 8080\n    schema: core_data\n    session_properties:                       # Optional\n      acc01.optimize_locality_enabled: false\n      optimize_hash_generation: true\n    source: trino-cli                         # Optional\n    user: admin\n    verify: /path/to/your/ca.crt              # Optional\n  # Weaviate\n  WEAVIATE_ENDPOINT: https://some-endpoint.weaviate.network\n  WEAVIATE_INSTANCE_API_KEY: YOUR-WEAVIATE-API-KEY\n  WEAVIATE_INFERENCE_API_KEY: YOUR-OPENAI-API-KEY\n  WEAVIATE_COLLECTION: collectionn_name\n", "file_path": "pipelines/mlops_project/io_config.yaml", "language": "yaml", "type": "pipeline", "uuid": "mlops_project/io_config"}, "pipelines/mlops_project/interactions/__init__.py:pipeline:python:mlops project/interactions/  init  ": {"content": "", "file_path": "pipelines/mlops_project/interactions/__init__.py", "language": "python", "type": "pipeline", "uuid": "mlops_project/interactions/__init__"}, "pipelines/mlops_project/charts/__init__.py:pipeline:python:mlops project/charts/  init  ": {"content": "", "file_path": "pipelines/mlops_project/charts/__init__.py", "language": "python", "type": "pipeline", "uuid": "mlops_project/charts/__init__"}, "pipelines/mlops_project/pipelines/__init__.py:pipeline:python:mlops project/pipelines/  init  ": {"content": "", "file_path": "pipelines/mlops_project/pipelines/__init__.py", "language": "python", "type": "pipeline", "uuid": "mlops_project/pipelines/__init__"}, "pipelines/mlops_project/pipelines/example_pipeline/metadata.yaml:pipeline:yaml:mlops project/pipelines/example pipeline/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - fill_in_missing_values\n  name: load_titanic\n  status: not_executed\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_titanic\n- all_upstream_blocks_executed: true\n  downstream_blocks:\n  - export_titanic_clean\n  name: fill_in_missing_values\n  status: not_executed\n  type: transformer\n  upstream_blocks:\n  - load_titanic\n  uuid: fill_in_missing_values\n- all_upstream_blocks_executed: true\n  downstream_blocks: []\n  name: export_titanic_clean\n  status: not_executed\n  type: data_exporter\n  upstream_blocks:\n  - fill_in_missing_values\n  uuid: export_titanic_clean\nname: example_pipeline\ntype: python\nuuid: example_pipeline\nwidgets: []\n", "file_path": "pipelines/mlops_project/pipelines/example_pipeline/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "mlops_project/pipelines/example_pipeline/metadata"}, "pipelines/mlops_project/pipelines/example_pipeline/__init__.py:pipeline:python:mlops project/pipelines/example pipeline/  init  ": {"content": "", "file_path": "pipelines/mlops_project/pipelines/example_pipeline/__init__.py", "language": "python", "type": "pipeline", "uuid": "mlops_project/pipelines/example_pipeline/__init__"}, "pipelines/mlops_project/utils/__init__.py:pipeline:python:mlops project/utils/  init  ": {"content": "", "file_path": "pipelines/mlops_project/utils/__init__.py", "language": "python", "type": "pipeline", "uuid": "mlops_project/utils/__init__"}, "pipelines/mlops_project/extensions/__init__.py:pipeline:python:mlops project/extensions/  init  ": {"content": "", "file_path": "pipelines/mlops_project/extensions/__init__.py", "language": "python", "type": "pipeline", "uuid": "mlops_project/extensions/__init__"}, "pipelines/mlops_project/scratchpads/__init__.py:pipeline:python:mlops project/scratchpads/  init  ": {"content": "", "file_path": "pipelines/mlops_project/scratchpads/__init__.py", "language": "python", "type": "pipeline", "uuid": "mlops_project/scratchpads/__init__"}, "pipelines/mlops_project/transformers/__init__.py:pipeline:python:mlops project/transformers/  init  ": {"content": "", "file_path": "pipelines/mlops_project/transformers/__init__.py", "language": "python", "type": "pipeline", "uuid": "mlops_project/transformers/__init__"}, "pipelines/mlops_project/transformers/fill_in_missing_values.py:pipeline:python:mlops project/transformers/fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "pipelines/mlops_project/transformers/fill_in_missing_values.py", "language": "python", "type": "pipeline", "uuid": "mlops_project/transformers/fill_in_missing_values"}, "pipelines/mlops_project/data_exporters/__init__.py:pipeline:python:mlops project/data exporters/  init  ": {"content": "", "file_path": "pipelines/mlops_project/data_exporters/__init__.py", "language": "python", "type": "pipeline", "uuid": "mlops_project/data_exporters/__init__"}, "pipelines/mlops_project/data_exporters/export_titanic_clean.py:pipeline:python:mlops project/data exporters/export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "pipelines/mlops_project/data_exporters/export_titanic_clean.py", "language": "python", "type": "pipeline", "uuid": "mlops_project/data_exporters/export_titanic_clean"}, "pipelines/mlops_project/custom/__init__.py:pipeline:python:mlops project/custom/  init  ": {"content": "", "file_path": "pipelines/mlops_project/custom/__init__.py", "language": "python", "type": "pipeline", "uuid": "mlops_project/custom/__init__"}, "pipelines/mlops_project/data_loaders/load_titanic.py:pipeline:python:mlops project/data loaders/load titanic": {"content": "import pandas as pd\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "pipelines/mlops_project/data_loaders/load_titanic.py", "language": "python", "type": "pipeline", "uuid": "mlops_project/data_loaders/load_titanic"}, "pipelines/mlops_project/data_loaders/__init__.py:pipeline:python:mlops project/data loaders/  init  ": {"content": "", "file_path": "pipelines/mlops_project/data_loaders/__init__.py", "language": "python", "type": "pipeline", "uuid": "mlops_project/data_loaders/__init__"}}, "custom_block_template": {}, "mage_template": {"data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}